{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c19eeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/Jwink3101/parmapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dd92eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..')\n",
    "import cadence as cd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ed6c402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parmapper import parmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a30d6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = parmap(\n",
    "#     do_something,\n",
    "#     cd.scan_iter(cd.txt),\n",
    "#     N=2,\n",
    "#     progress=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13214e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,tempfile,multiprocessing as mp\n",
    "import numpy as np,pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# default num proc is?\n",
    "DEFAULT_NUM_PROC = mp.cpu_count() - 1\n",
    "\n",
    "# boost to max if just 2? (hack for google colab/cloud context)\n",
    "if mp.cpu_count()==2: DEFAULT_NUM_PROC=2\n",
    "\n",
    "\n",
    "def pmap_iter(func, objs, args=[], kwargs={}, num_proc=DEFAULT_NUM_PROC, use_threads=False, progress=True, progress_pos=0,desc=None, **y):\n",
    "    \"\"\"\n",
    "    Yields results of func(obj) for each obj in objs\n",
    "    Uses multiprocessing.Pool(num_proc) for parallelism.\n",
    "    If use_threads, use ThreadPool instead of Pool.\n",
    "    Results in any order.\n",
    "    \"\"\"\n",
    "\n",
    "    # check num proc\n",
    "    num_cpu = mp.cpu_count()\n",
    "    if num_proc>num_cpu: num_proc=num_cpu\n",
    "    if num_proc>len(objs): num_proc=len(objs)\n",
    "\n",
    "    # if parallel\n",
    "    if not desc: desc=f'Mapping {func.__name__}()'\n",
    "    if desc and num_cpu>1: desc=f'{desc} [x{num_proc}]'\n",
    "    if num_proc>1 and len(objs)>1:\n",
    "\n",
    "        # real objects\n",
    "        objects = [(func,obj,args,kwargs) for obj in objs]\n",
    "\n",
    "        # create pool\n",
    "        pool=mp.Pool(num_proc) if not use_threads else mp.pool.ThreadPool(num_proc)\n",
    "\n",
    "        # yield iter\n",
    "        iterr = pool.imap(_pmap_do, objects)\n",
    "\n",
    "        for res in tqdm(iterr,total=len(objects),desc=desc,position=progress_pos) if progress else iterr:\n",
    "            yield res\n",
    "\n",
    "        # Close the pool?\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    else:\n",
    "        # yield\n",
    "        for obj in (tqdm(objs,desc=desc,position=progress_pos) if progress else objs):\n",
    "            yield func(obj,*args,**kwargs)\n",
    "\n",
    "def _pmap_do(inp):\n",
    "    func,obj,args,kwargs = inp\n",
    "    return func(obj,*args,**kwargs)\n",
    "\n",
    "def pmap(*x,**y):\n",
    "    \"\"\"\n",
    "    Non iterator version of pmap_iter\n",
    "    \"\"\"\n",
    "    # return as list\n",
    "    return list(pmap_iter(*x,**y))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Pandas functions\n",
    "\"\"\"\n",
    "\n",
    "def pmap_df(df, func, num_proc=DEFAULT_NUM_PROC):\n",
    "    df_split = np.array_split(df, num_proc)\n",
    "    df = pd.concat(pmap(func, df_split, num_proc=num_proc))\n",
    "    return df\n",
    "\n",
    "\n",
    "def pmap_groups(func,df_grouped,use_cache=False,num_proc=DEFAULT_NUM_PROC,shuffle=True,**attrs):\n",
    "\n",
    "\n",
    "    # get index/groupby col name(s)\n",
    "    group_key=df_grouped.grouper.names\n",
    "    # if not using cache\n",
    "    # if not use_cache or attrs.get('num_proc',1)<2:\n",
    "    if not use_cache or len(df_grouped)<2 or num_proc<2:\n",
    "        objs=[\n",
    "            (func,group_df,group_key,group_name)\n",
    "            for group_name,group_df in df_grouped\n",
    "        ]\n",
    "    else:\n",
    "        objs=[]\n",
    "        groups=list(df_grouped)\n",
    "        for i,(group_name,group_df) in enumerate(tqdm(groups,desc='Preparing input')):\n",
    "            # print([i,group_name,tmp_path,group_df])\n",
    "            if use_cache:\n",
    "                tmpdir=tempfile.mkdtemp()\n",
    "                tmp_path = os.path.join(tmpdir, str(i)+'.pkl')\n",
    "                group_df.to_pickle(tmp_path)\n",
    "            objs+=[(func,tmp_path if use_cache else group_df,group_key,group_name)]\n",
    "\n",
    "    # desc?\n",
    "    if not attrs.get('desc'): attrs['desc']=f'Mapping {func.__name__}'\n",
    "\n",
    "    if shuffle: random.shuffle(objs)\n",
    "\n",
    "    return pd.concat(\n",
    "        pmap(\n",
    "            _do_pmap_group,\n",
    "            objs,\n",
    "            num_proc=num_proc,\n",
    "            **attrs\n",
    "        )\n",
    "    ).set_index(group_key)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _do_pmap_group(obj,*x,**y):\n",
    "    # unpack\n",
    "    func,group_df,group_key,group_name = obj\n",
    "    # load from cache?\n",
    "    if type(group_df)==str:\n",
    "        group_df=pd.read_pickle(group_df)\n",
    "    # run func\n",
    "    outdf=func(group_df,*x,**y)\n",
    "    # annotate with groupnames on way out\n",
    "    if type(group_name) not in {list,tuple}:group_name=[group_name]\n",
    "    for x,y in zip(group_key,group_name):\n",
    "        outdf[x]=y\n",
    "    # return\n",
    "    return outdf\n",
    "\n",
    "\n",
    "\n",
    "def pmap_apply_cols(func, df, lim=None, **y):\n",
    "    cols=list(df.columns)[:lim]\n",
    "    new_seriess = pmap(\n",
    "        func,\n",
    "        [df[col] for col in cols],\n",
    "        **y\n",
    "    )\n",
    "    odf=pd.DataFrame(dict(zip(cols,new_seriess)))\n",
    "    return odf\n",
    "\n",
    "def pmap_iter_gen_process(q, iolock):\n",
    "    while True:\n",
    "        stuff = q.get()\n",
    "        if stuff is None:\n",
    "            break\n",
    "        with iolock:\n",
    "            print(\"processing\", stuff)\n",
    "#         sleep(stuff)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a27fa2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "from multiprocessing import Process, Queue\n",
    "import sys\n",
    "import logging\n",
    "import traceback\n",
    "import inspect\n",
    "\n",
    "#TODO: Make these more unique\n",
    "STOP = \"STOP\"\n",
    "SHUTDOWN = \"SHUTDOWN\"\n",
    "SHUTDOWN_LAST = \"SHUTDOWN_LAST\"\n",
    "\n",
    "log = None\n",
    "\n",
    "class Task:\n",
    "    def __init__(self, id, fn, inputQueue, outputQueue, multiplicity):\n",
    "        self.id = id\n",
    "        self.fn = fn\n",
    "        self.inputQueue = inputQueue\n",
    "        self.outputQueue = outputQueue\n",
    "        self.multiplicity = multiplicity\n",
    "\n",
    "    def start(self):\n",
    "        self.process = Process(target=self.main, args=(self.inputQueue, self.outputQueue))\n",
    "        self.process.start()\n",
    "\n",
    "    def main(self, inputQueue, outputQueue):\n",
    "        self.inputQueue = inputQueue\n",
    "        self.outputQueue = outputQueue\n",
    "\n",
    "        if inspect.isfunction(self.fn):\n",
    "            logger = logging.getLogger(str(self.id) + \":\" +\n",
    "                    self.fn.__name__)\n",
    "        else:\n",
    "            logger = logging.getLogger(str(self.id) + \":\" +\n",
    "                    type(self.fn).__name__)\n",
    "        global log\n",
    "        log = lambda a: logger.debug(a)\n",
    "\n",
    "        try:\n",
    "            if hasattr(self.fn, \"init\"):\n",
    "                self.fn.init()\n",
    "\n",
    "            log(\"Running\")\n",
    "\n",
    "            while True:\n",
    "                input = self.inputQueue.get()\n",
    "                log(\"Input is {}\".format(input))\n",
    "                if input == SHUTDOWN: break\n",
    "                if input == SHUTDOWN_LAST:\n",
    "                    self.outputQueue.put(STOP)\n",
    "                    break\n",
    "                if input == STOP: \n",
    "                    for i in range(self.multiplicity-1):\n",
    "                        self.inputQueue.put(SHUTDOWN)\n",
    "                    self.inputQueue.put(SHUTDOWN_LAST)\n",
    "                    continue\n",
    "\n",
    "                result = self.fn(input)\n",
    "                if inspect.isgenerator(result):\n",
    "                    for x in result:\n",
    "                        if x == STOP: \n",
    "                            self.inputQueue.put(STOP)\n",
    "                            break\n",
    "                        self.outputQueue.put(x)\n",
    "                else:\n",
    "                    if result == STOP: \n",
    "                        self.inputQueue.put(STOP)\n",
    "                    else:\n",
    "                        self.outputQueue.put(result)\n",
    "                    \n",
    "\n",
    "            log(\"Shutting down\")\n",
    "            if hasattr(self.fn, \"shutdown\"):\n",
    "                self.fn.shutdown()\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "        except Exception:\n",
    "            print(\"For {}\".format(self.fn))\n",
    "            raise\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "    def __init__(self):\n",
    "        self.tasks = []\n",
    "        self.inputQueue = Queue(1)\n",
    "        self.outputQueue = Queue(1)\n",
    "        self.nextId = 1\n",
    "\n",
    "    def run(self, arg = None):\n",
    "\n",
    "        for task in self.tasks:\n",
    "            task.start()\n",
    "\n",
    "        self.inputQueue.put(arg)\n",
    "        while True:\n",
    "            x = self.outputQueue.get()\n",
    "            if x == STOP: break\n",
    "\n",
    "    def add(self, fn, fanOut=1):\n",
    "        inputQueue = self.inputQueue\n",
    "        outputQueue = self.outputQueue\n",
    "        if len(self.tasks):\n",
    "            inputQueue = Queue(2)\n",
    "            for task in self.tasks:\n",
    "                if task.outputQueue == self.outputQueue:\n",
    "                    task.outputQueue = inputQueue\n",
    "\n",
    "        for i in range(fanOut):\n",
    "            task = Task(self.nextId, fn, inputQueue, outputQueue, fanOut)\n",
    "            self.nextId += 1\n",
    "            self.tasks.append(task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39b39cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def do_something(obj):\n",
    "    for n in range(obj):\n",
    "        yield n\n",
    "    #print(len(obj))\n",
    "    #time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fab6cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = parmap(\n",
    "    do_something,\n",
    "    cd.scan_iter(cd.txt),\n",
    "    N=2,\n",
    "    progress=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110ae8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterating over line scansions [x4]:   0%|          | 0/7 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ryan/miniconda3/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/home/ryan/miniconda3/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/home/ryan/miniconda3/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/home/ryan/miniconda3/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "TypeError: cannot pickle 'generator' object\n",
      "TypeError: cannot pickle 'generator' object\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ryan/miniconda3/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/home/ryan/miniconda3/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "TypeError: cannot pickle 'generator' object\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ryan/miniconda3/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/home/ryan/miniconda3/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "TypeError: cannot pickle 'generator' object\n",
      "Iterating over line scansions [x4]: 100%|██████████| 7/7 [00:00<00:00, 53.57it/s]Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ryan/miniconda3/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/home/ryan/miniconda3/lib/python3.8/multiprocessing/queues.py\", line 239, in _feed\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/home/ryan/miniconda3/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/home/ryan/miniconda3/lib/python3.8/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "TypeError: cannot pickle 'generator' object\n",
      "TypeError: cannot pickle 'generator' object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in data:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76b8d4ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-7-0deb9470836f>, line 36)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-0deb9470836f>\"\u001b[0;36m, line \u001b[0;32m36\u001b[0m\n\u001b[0;31m    q.put(stuff)  # blocks until q below its max size\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def pmap_iter_gen(\n",
    "        func,\n",
    "        objs,\n",
    "        args=[],\n",
    "        kwargs={},\n",
    "        num_proc=DEFAULT_NUM_PROC,\n",
    "        use_threads=False,\n",
    "        progress=True,\n",
    "        progress_pos=0,\n",
    "        desc=None,\n",
    "        **y):\n",
    "    \"\"\"\n",
    "    Yields results of func(obj) for each obj in objs\n",
    "    Uses multiprocessing.Pool(num_proc) for parallelism.\n",
    "    If use_threads, use ThreadPool instead of Pool.\n",
    "    Results in any order.\n",
    "    \"\"\"\n",
    "\n",
    "    # check num proc\n",
    "    num_cpu = mp.cpu_count()\n",
    "    if num_proc>num_cpu: num_proc=num_cpu\n",
    "    if num_proc>len(objs): num_proc=len(objs)\n",
    "\n",
    "    # if parallel\n",
    "    if not desc: desc=f'Mapping {func.__name__}()'\n",
    "    if desc and num_cpu>1: desc=f'{desc} [x{num_proc}]'\n",
    "    if num_proc>1 and len(objs)>1:\n",
    "        q = mp.Queue(maxsize=NCORE)\n",
    "        iolock = mp.Lock()\n",
    "        pool = mp.Pool(\n",
    "            num_cpu,\n",
    "            initializer=process,\n",
    "            initargs=(q, iolock)\n",
    "        )\n",
    "        for stuff in range(20):\n",
    "        q.put(stuff)  # blocks until q below its max size\n",
    "        with iolock:\n",
    "        print(\"queued\", stuff)\n",
    "        for _ in range(NCORE):  # tell workers we're done\n",
    "        q.put(None)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "        \n",
    "\n",
    "        # real objects\n",
    "        objects = [(func,obj,args,kwargs) for obj in objs]\n",
    "\n",
    "        # create pool\n",
    "        pool=mp.Pool(num_proc) if not use_threads else mp.pool.ThreadPool(num_proc)\n",
    "\n",
    "        # yield iter\n",
    "        iterr = pool.imap(_pmap_do, objects)\n",
    "\n",
    "        for res in tqdm(iterr,total=len(objects),desc=desc,position=progress_pos) if progress else iterr:\n",
    "            yield res\n",
    "\n",
    "        # Close the pool?\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    else:\n",
    "        # yield\n",
    "        for obj in (tqdm(objs,desc=desc,position=progress_pos) if progress else objs):\n",
    "            yield func(obj,*args,**kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "229d47e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process(q, iolock):\n",
    "#     from time import sleep\n",
    "#     while True:\n",
    "#         stuff = q.get()\n",
    "#         if stuff is None:\n",
    "#             break\n",
    "#         with iolock:\n",
    "#             print(\"processing\", stuff)\n",
    "#         sleep(stuff)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     q = mp.Queue(maxsize=NCORE)\n",
    "#     iolock = mp.Lock()\n",
    "#     pool = mp.Pool(NCORE, initializer=process, initargs=(q, iolock))\n",
    "#     for stuff in range(20):\n",
    "#         q.put(stuff)  # blocks until q below its max size\n",
    "#         with iolock:\n",
    "#             print(\"queued\", stuff)\n",
    "#     for _ in range(NCORE):  # tell workers we're done\n",
    "#         q.put(None)\n",
    "#     pool.close()\n",
    "#     pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b50817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmap_iter_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31edd29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
