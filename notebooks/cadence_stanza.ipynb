{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ecd93b1-4a57-4717-921d-fba48d082c2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stanza integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d458e262-ffb6-4df5-a819-36328b99ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_start_method('spawn')\n",
    "\n",
    "import sys; sys.path.insert(0,'/Users/ryan/github/prosodic/')\n",
    "import sys; sys.path.insert(0,'/Users/ryan/github/cadence/')\n",
    "from cadence.parsers.mtree import MetricalTree,DependencyTree,DependencyTreeParser,MetricalTreeParser\n",
    "from cadence.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "652013e9-44e0-4a25-8986-b314b8fa78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=\"\"\"\n",
    "\n",
    "Turning and turning in the widening gyre, \n",
    "the falcon cannot hear the falconer.\n",
    "Things fall apart; the centre cannot hold.\n",
    "   Mere anarchy is loosed upon the world. \n",
    "\n",
    "The blood-dimmed tide is loosed, and everywhere\n",
    "The ceremony of innocence is drowned;\n",
    "\n",
    "\n",
    "\n",
    "The best lack all conviction, while the worst   \n",
    "Are full of passionate \n",
    "intensity.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ba4b266-0dd4-41f2-bda0-5dc07121322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_paras_df(txt_or_df,**kwargs):\n",
    "    if type(txt_or_df)==str:\n",
    "        for para_i,para_d in enumerate(tokenize_paras_ld(txt_or_df,**kwargs)):\n",
    "            yield tokenize_sentwords(para_d['para_str'],para_i=para_d['para_i'])\n",
    "    elif type(txt_or_df)==pd.DataFrame:\n",
    "        for para_i,para_df in sorted(txt_or_df.groupby('para_i')):\n",
    "            yield para_df\n",
    "            \n",
    "def get_num_paras(txt_or_df,**kwargs):\n",
    "    if type(txt_or_df)==str:\n",
    "        return len([pstr.strip() for pstr in txt_or_df.split(SEP_PARA)])\n",
    "    elif type(txt_or_df)==pd.DataFrame:\n",
    "        return len(set(getcol(txt_or_df,'para_i')))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e967511-92ee-4cd7-8682-dd627dc9dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_parse_nlp_docs(\n",
    "        docs,\n",
    "        nlp=None,\n",
    "        num_proc=1,\n",
    "        **kwargs):\n",
    "    ## nlp\n",
    "    # if not len(docs): yield docs\n",
    "    if num_proc>1:\n",
    "        with mp.Pool(num_proc) as pool: # This is the fastest. joblib(thread, mp) experimented.\n",
    "            yield from pool.imap(nlp, docs)\n",
    "    else:\n",
    "        for doc in docs:\n",
    "            yield nlp(doc) if nlp is not None else doc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efc65156-db7e-47aa-977b-54c573149308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_nlp_doc(tokdf,doc,**kwargs):\n",
    "    sents=doc.sentences\n",
    "    ld=[]\n",
    "    cols_done=set(tokdf.columns)\n",
    "    for sent_i, sent in enumerate(sents):\n",
    "        for word_i,word in enumerate(sent.tokens):\n",
    "            feats=word.to_dict()[0]\n",
    "            statd=dict((f'word_{k}',v) for k,v in feats.items() if k not in badcols)\n",
    "            for feat in feats.get('feats','').split('|'):\n",
    "                if not feat: continue\n",
    "                fk,fv=feat.split('=',1)\n",
    "                statd[fk]=fv\n",
    "            \n",
    "            dx={\n",
    "                'sent_i': sent.id+1,\n",
    "                'word_i': word_i+1,\n",
    "                **statd\n",
    "            }\n",
    "            ld.append(dx)\n",
    "    df=pd.DataFrame(ld).fillna('')\n",
    "    joiner=['sent_i','word_i']\n",
    "    ocols=(set(df.columns)-set(tokdf.columns))|set(joiner)\n",
    "    return tokdf.merge(df[ocols],on=joiner,how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "619aa850-c3f8-458d-83e4-cd05275f0603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_iter_nlp(\n",
    "        txt,\n",
    "        nlp=None,\n",
    "        paras_lim=None,\n",
    "        progress=True,\n",
    "        num_proc=1,\n",
    "    \n",
    "        lang=DEFAULT_LANG,\n",
    "        paras_shuffle=False,\n",
    "        \n",
    "        postag=False,\n",
    "        constituency=False,\n",
    "        depparse=False,\n",
    "        syllabify=True,\n",
    "    \n",
    "        **kwargs):\n",
    "\n",
    "    ## prep documents\n",
    "    paras_ld=tokenize_paras_ld(txt)\n",
    "    if paras_shuffle: random.shuffle(paras_ld)\n",
    "    if paras_lim: paras_ld=paras_ld[:paras_lim]\n",
    "    \n",
    "    para_dfs=[\n",
    "        tokenize_sentwords(para_d['para_str'],para_i=para_d['para_i'])\n",
    "        for para_d in tqdm(paras_ld,desc='Tokenizing paragraphs')\n",
    "    ]\n",
    "    \n",
    "    para_doclls=[\n",
    "        tokenize_sentwords_ll(para_df)\n",
    "        for para_df in tqdm(para_dfs,desc='Tokenizing sentences and words')\n",
    "    ]\n",
    "    \n",
    "    processors=get_processors(\n",
    "        postag=postag,\n",
    "        constituency=constituency,\n",
    "        depparse=depparse,\n",
    "    )\n",
    "    \n",
    "    if processors and nlp is None:\n",
    "        nlp = get_nlp(\n",
    "            lang=lang,\n",
    "            pretokenized=True,\n",
    "            processors=processors\n",
    "        )\n",
    "        \n",
    "    # iter docs\n",
    "    doc_iter = iter_parse_nlp_docs(\n",
    "        para_doclls,\n",
    "        nlp=nlp,\n",
    "        lang=lang,\n",
    "        constituency=constituency,\n",
    "        depparse=depparse,\n",
    "        num_proc=num_proc,\n",
    "        progress=False\n",
    "    )\n",
    "    \n",
    "    oiterr=zip(paras_ld,para_dfs,doc_iter)\n",
    "    \n",
    "    if progress:\n",
    "        oiterr=tqdm(\n",
    "            oiterr,\n",
    "            total=len(para_doclls),\n",
    "            desc=f'Tokenizing NLP [x{num_proc}]'\n",
    "        )\n",
    "    \n",
    "    # yield from oiterr\n",
    "    for para_d,para_tokdf,para_doc in oiterr:\n",
    "        #if postag or constituency or depparse:\n",
    "        if processors:\n",
    "            para_tokdf = tokenize_nlp_doc(para_tokdf, para_doc, **kwargs)\n",
    "        \n",
    "        if constituency:\n",
    "            para_tokdf = tokenize_constituency(para_tokdf,para_doc,**kwargs)\n",
    "        \n",
    "        if syllabify:\n",
    "            para_tokdf=syllabify_df(para_tokdf,**kwargs)\n",
    "        \n",
    "        yield setindex(para_tokdf.assign(para_i=para_d['para_i']))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "510525cc-b641-4d96-8f0e-c73376e06cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oiter=scan_iter_nlp(\n",
    "#     txt,\n",
    "#     paras_lim=100,\n",
    "#     # syllabify=True,\n",
    "#     postag=True,\n",
    "#     depparse=True,\n",
    "#     constituency=True,\n",
    "#     syllabify=True,\n",
    "#     num_proc=1\n",
    "# )\n",
    "# for scanned_para_df in oiter: pass\n",
    "# scanned_para_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17127420-1221-4cb7-a16e-57ac89279e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_iter(txt,groupby='para',**kwargs):\n",
    "    if 'syllabify' not in kwargs: kwargs['syllabify']=True\n",
    "    for df_para in scan_iter_nlp(txt,**kwargs):\n",
    "        \n",
    "        grpr=None\n",
    "        if groupby=='sent':\n",
    "            grpr='sent_i'\n",
    "        elif groupby=='sentpart':\n",
    "            grpr=['sent_i','sentpart_i']\n",
    "        elif groupby=='word':\n",
    "            grpr=['sent_i','word_i']\n",
    "        elif groupby=='syll':\n",
    "            grpr=['sent_i','word_i','word_ipa_i','syll_i']\n",
    "            \n",
    "        if grpr is None:\n",
    "            yield df_para\n",
    "        else:\n",
    "            for gi,dfg in sorted(df_para.groupby(grpr)):\n",
    "                yield dfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb5cbadc-b62c-4446-9446-83e83a5b7a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing paragraphs: 100%|████████████████████████████████████████████████| 3/3 [00:00<00:00, 340.60it/s]\n",
      "Tokenizing sentences and words: 100%|███████████████████████████████████████| 3/3 [00:00<00:00, 794.93it/s]\n",
      "Tokenizing NLP [x1]:   0%|                                                           | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>is_funcword</th>\n",
       "      <th>is_heavy</th>\n",
       "      <th>is_light</th>\n",
       "      <th>is_peak</th>\n",
       "      <th>is_stressed</th>\n",
       "      <th>is_trough</th>\n",
       "      <th>is_unstressed</th>\n",
       "      <th>prom_strength</th>\n",
       "      <th>prom_stress</th>\n",
       "      <th>prom_weight</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>para_i</th>\n",
       "      <th>sent_i</th>\n",
       "      <th>sentpart_i</th>\n",
       "      <th>line_i</th>\n",
       "      <th>word_i</th>\n",
       "      <th>word_pref</th>\n",
       "      <th>word_str</th>\n",
       "      <th>word_ipa_i</th>\n",
       "      <th>word_ipa</th>\n",
       "      <th>syll_i</th>\n",
       "      <th>syll_str</th>\n",
       "      <th>syll_ipa</th>\n",
       "      <th>syll_stress</th>\n",
       "      <th>syll_weight</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"13\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"13\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"13\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"13\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\"></th>\n",
       "      <th rowspan=\"2\" valign=\"top\">Turning</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">'tɛː.nɪŋ</th>\n",
       "      <th>1</th>\n",
       "      <th>Tur</th>\n",
       "      <th>'tɛː</th>\n",
       "      <th>P</th>\n",
       "      <th></th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>ning</th>\n",
       "      <th>nɪŋ</th>\n",
       "      <th>U</th>\n",
       "      <th></th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>1</th>\n",
       "      <th>ænd</th>\n",
       "      <th>1</th>\n",
       "      <th>and</th>\n",
       "      <th>ænd</th>\n",
       "      <th>U</th>\n",
       "      <th></th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th rowspan=\"2\" valign=\"top\"></th>\n",
       "      <th rowspan=\"2\" valign=\"top\">turning</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">'tɛː.nɪŋ</th>\n",
       "      <th>1</th>\n",
       "      <th>tur</th>\n",
       "      <th>'tɛː</th>\n",
       "      <th>P</th>\n",
       "      <th></th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>ning</th>\n",
       "      <th>nɪŋ</th>\n",
       "      <th>U</th>\n",
       "      <th></th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th rowspan=\"2\" valign=\"top\"></th>\n",
       "      <th rowspan=\"2\" valign=\"top\">in</th>\n",
       "      <th>1</th>\n",
       "      <th>ɪn</th>\n",
       "      <th>1</th>\n",
       "      <th>in</th>\n",
       "      <th>ɪn</th>\n",
       "      <th>U</th>\n",
       "      <th></th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>'ɪn</th>\n",
       "      <th>1</th>\n",
       "      <th>in</th>\n",
       "      <th>'ɪn</th>\n",
       "      <th>P</th>\n",
       "      <th></th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>1</th>\n",
       "      <th>ðə</th>\n",
       "      <th>1</th>\n",
       "      <th>the</th>\n",
       "      <th>ðə</th>\n",
       "      <th>U</th>\n",
       "      <th></th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">6</th>\n",
       "      <th rowspan=\"3\" valign=\"top\"></th>\n",
       "      <th rowspan=\"3\" valign=\"top\">widening</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">'waɪ.də.nɪŋ</th>\n",
       "      <th>1</th>\n",
       "      <th>wi</th>\n",
       "      <th>'waɪ</th>\n",
       "      <th>P</th>\n",
       "      <th></th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>de</th>\n",
       "      <th>də</th>\n",
       "      <th>U</th>\n",
       "      <th></th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>ning</th>\n",
       "      <th>nɪŋ</th>\n",
       "      <th>U</th>\n",
       "      <th></th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">7</th>\n",
       "      <th rowspan=\"2\" valign=\"top\"></th>\n",
       "      <th rowspan=\"2\" valign=\"top\">gyre,</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">'dʒaɪ.ʌ</th>\n",
       "      <th>1</th>\n",
       "      <th>gy</th>\n",
       "      <th>'dʒaɪ</th>\n",
       "      <th>P</th>\n",
       "      <th></th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>re,</th>\n",
       "      <th>ʌ</th>\n",
       "      <th>U</th>\n",
       "      <th></th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                   is_funcword  ...  prom_weight\n",
       "para_i sent_i sentpart_i line_i word_i word_pref word_str word_ipa_i word_ipa    syll_i syll_str syll_ipa syll_stress syll_weight               ...             \n",
       "1      1      1          1      1                Turning  1          'tɛː.nɪŋ    1      Tur      'tɛː     P                                  0  ...          NaN\n",
       "                                                                                 2      ning     nɪŋ      U                                  0  ...          NaN\n",
       "                                2                and      1          ænd         1      and      ænd      U                                  1  ...          NaN\n",
       "                                3                turning  1          'tɛː.nɪŋ    1      tur      'tɛː     P                                  0  ...          NaN\n",
       "                                                                                 2      ning     nɪŋ      U                                  0  ...          NaN\n",
       "                                4                in       1          ɪn          1      in       ɪn       U                                  1  ...          NaN\n",
       "                                                          2          'ɪn         1      in       'ɪn      P                                  0  ...          NaN\n",
       "                                5                the      1          ðə          1      the      ðə       U                                  1  ...          NaN\n",
       "                                6                widening 1          'waɪ.də.nɪŋ 1      wi       'waɪ     P                                  0  ...          NaN\n",
       "                                                                                 2      de       də       U                                  0  ...          NaN\n",
       "                                                                                 3      ning     nɪŋ      U                                  0  ...          NaN\n",
       "                                7                gyre,    1          'dʒaɪ.ʌ     1      gy       'dʒaɪ    P                                  0  ...          NaN\n",
       "                                                                                 2      re,      ʌ        U                                  0  ...          NaN\n",
       "\n",
       "[13 rows x 10 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(scan_iter(txt,groupby='sentpart'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35e6972-6e6e-4311-9020-fbc2a21a64ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57b725e-c1e0-4cbc-8011-07dd9ac32d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cba2cc1-30a2-421a-b364-d10adbc4ae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "    \n",
    "ld=tokenize_paras_ld(txt)\n",
    "ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d144de47-f2d7-4717-a25c-faac9b18f810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce557a6-9517-42d6-88d8-3fc5475469e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df1340-6bc4-48eb-83a1-b4a099b8166f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94da742c-d194-415d-a70f-0e3dfeceda2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "list(iter_paras_df(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e4774e-2c0b-4b1c-9f26-5915f996e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "txtdf=pd.concat(iter_paras_df(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a0f7dd-8c6d-4b1a-a309-4acaa3924611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# txtdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecf494b-7566-4145-a2b4-ec8e2f940b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "list(iter_paras_df(txtdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a77a9b7-a4e7-4233-b4b9-035bc86bc8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab607e-f05a-4c6e-8bfb-ec5e7a9440af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentwords_ll(tokdf,**kwargs):\n",
    "    if not len(tokdf): return []\n",
    "    return [list(getcol(sdf,'word_str')) for si,sdf in sorted(tokdf.groupby('sent_i'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b36808-dfda-4665-8456-a809743fee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_paras_nlp(\n",
    "        txt_or_tokdf,\n",
    "        nlp=None,\n",
    "        constituency=False,\n",
    "        depparse=False,\n",
    "        progress=True,\n",
    "        syllabify=False,\n",
    "        num_proc=1,\n",
    "        lang=DEFAULT_LANG,\n",
    "        **kwargs):\n",
    "    \n",
    "    # get nlp\n",
    "    if nlp is None:\n",
    "        processors=get_processors(constituency=constituency,depparse=depparse)\n",
    "        print(processors)\n",
    "        nlp=get_nlp(lang=lang, procesors=processors)\n",
    "    \n",
    "    oiterr=iter_paras_df(txt_or_tokdf,**kwargs)\n",
    "    if progress: oiterr=tqdm(oiterr,total=get_num_paras(txt_or_tokdf))\n",
    "    for para_df in oiterr:\n",
    "        sentwords_ll = tokenize_sentwords_ll(para_df)\n",
    "        doc = nlp(sentwords_ll)\n",
    "        para_tokdf = tokenize_nlp_doc(para_df, doc, lang=lang, **kwargs)\n",
    "        if constituency: para_tokdf = tokenize_constituency(para_tokdf,doc,**lang)\n",
    "        if syllabify: para_tokdf=syllabify_df(para_tokdf,**kwargs)\n",
    "        yield setindex(para_tokdf)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db58df70-0dca-44c7-ae61-ef7c3d72b30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pdf in iter_paras_nlp(txt): pass\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad312f-22d6-464e-bdbe-c2a5fcdf6dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def scan_iter_nlp(\n",
    "#         txt_or_tokdf,\n",
    "#         nlp=None,\n",
    "#         constituency=False,\n",
    "#         depparse=False,\n",
    "#         syllabify=False,\n",
    "#         num_proc=1,\n",
    "#         custom_tokenize=True,\n",
    "#         lang=DEFAULT_LANG,\n",
    "#         **kwargs):\n",
    "    \n",
    "#     # make orig tokdf\n",
    "#     tokdf=tokenize_parasentword(txt_or_tokdf,**kwargs) if type(txt_or_tokdf)==str else txt_or_tokdf    \n",
    "    \n",
    "#     # get nlp\n",
    "#     if nlp is None:\n",
    "#         processors=get_processors(constituency=constituency,depparse=depparse,**kwargs)\n",
    "#         nlp=get_nlp(lang=lang, procesors=processors,custom_tokenize=custom_tokenize)\n",
    "    \n",
    "#     objs=[(paradf,nlp) for para_i,paradf in sorted(tokdf.groupby('para_i'))]\n",
    "#     kwargs=dict(constituency=constituency,depparse=depparse,syllabify=syllabify,**kwargs)\n",
    "#     oiter=pmap_iter(do_scan_iter_nlp, objs, num_proc=num_proc, kwargs=kwargs)\n",
    "    \n",
    "#     yield from oiter\n",
    "    \n",
    "    \n",
    "    \n",
    "# def do_scan_iter_nlp(\n",
    "#         obj,\n",
    "#         constituency=False,\n",
    "#         depparse=False,\n",
    "#         syllabify=False,                 \n",
    "#         **kwargs):\n",
    "    \n",
    "#     tokdf,nlp=obj\n",
    "#     sentwords=tokenize_sentwords_ll(tokdf)\n",
    "#     doc=nlp(sentwords)\n",
    "#     para_str,para_doc=para_row['para_str'], para_row['para_doc']\n",
    "\n",
    "#     # add anno?\n",
    "#     if constituency: tokdf=tokenize_constituency(tokdf,para_doc,**kwargs)\n",
    "#     if depparse: tokdf=tokenize_deps(tokdf,para_doc,**kwargs)\n",
    "#     if syllabify: tokdf=syllabify_df(tokdf,**kwargs)\n",
    "\n",
    "#     for k in ['para_i']: tokdf[k]=para_row[k]\n",
    "\n",
    "#     # done\n",
    "#     odf=setindex(tokdf)\n",
    "#     odf.attrs=dict(para_row)\n",
    "#     return tokdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf22dc3-2c57-42f5-942d-9f3359807cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(scan_iter_nlp(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26ca80e-2226-4886-8095-7a0fe698a7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(scan_iter(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c2ef65-35ff-4db7-93b4-7c5252084ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ac0847-2bce-4109-af74-0f91d9bef6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=list(scan_iter(txt,num_proc=1,lim_paras=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e310ac22-f4f8-493b-a1c3-0575c664e233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0ef6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for para in scan_iter(txt,\n",
    "                      num_proc=1,\n",
    "                      lim_paras=None,\n",
    "                      shuffle=True,\n",
    "                      depparse=False,\n",
    "                      constituency=False): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3737adb2-5679-4ba0-88cf-cf95c0e3a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# para.tail(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e7ae8d-690b-4530-977a-d45b88c38e20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d9a62951c4de3cec93df06e5a8769682e2513316501195b5ad08e283a24e7b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "language": "python",
   "name": "python3811jvsc74a57bd08d9a62951c4de3cec93df06e5a8769682e2513316501195b5ad08e283a24e7b2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
