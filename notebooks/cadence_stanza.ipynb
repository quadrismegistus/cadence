{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ecd93b1-4a57-4717-921d-fba48d082c2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stanza integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d458e262-ffb6-4df5-a819-36328b99ac72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0,'/Users/ryan/github/prosodic/')\n",
    "import sys; sys.path.insert(0,'/Users/ryan/github/cadence/')\n",
    "from cadence.parsers.mtree import MetricalTree,DependencyTree,DependencyTreeParser,MetricalTreeParser\n",
    "from cadence.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99b931a3-188c-4847-994b-3bee1cc22b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg eBook of Pride and Prejudice, by Jane Austen\\r\\n\\r\\nThis eBook is for the use of anyone anywhere in the United States and\\r\\nmost other parts of the world at no cost and with almost no restrictions\\r\\nwhatsoever. You may copy it, give it away or re-use it under the terms\\r\\nof the Project Gutenberg License included with this eBook or online at\\r\\nwww.gutenberg.org. If you are not located in the United States, you\\r\\nwill have to check the laws of the country where you are located before\\r\\nusing this eBook.\\r\\n\\r\\nTitle: Pride and Prejudice\\r\\n\\r\\nAuthor: Jane Austen\\r\\n\\r\\nRelease Date: June, 1998 [eBook #1342]\\r\\n[Most recently updated: August 23, 2021]\\r\\n\\r\\nLanguage: English\\r\\n\\r\\nCharacter set encoding: UTF-8\\r\\n\\r\\nProduced by: Anonymous Volunteers and David Widger\\r\\n\\r\\n*** START OF THE PROJECT GUTENBERG EBOOK PRIDE AND PREJUDICE ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nTHERE IS AN ILLUSTRATED EDITION OF THIS TITLE WHICH MAY VIEWED AT EBOOK\\r\\n[# 42671 ]\\r\\n\\r\\ncover\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nPride and Prejudice\\r\\n\\r\\nBy Jane Austen\\r\\n\\r\\nCONTENTS\\r\\n\\r\\n  C'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "novel = requests.get('https://www.gutenberg.org/files/1342/1342-0.txt').content.decode('utf-8')\n",
    "novel[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c879ff07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%timeit\n",
    "# ld=tokenize_paras_ld(novel,progress=True)\n",
    "# ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "652013e9-44e0-4a25-8986-b314b8fa78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=\"\"\"\n",
    "\n",
    "Turning, and turning in the widening gyre, \n",
    "the falcon cannot hear the falconer.\n",
    "Things fall apart; the centre cannot hold.\n",
    "   Mere anarchy is loosed upon the world. \n",
    "\n",
    "The blood-dimmed tide is loosed, and everywhere\n",
    "The ceremony of innocence is drowned;\n",
    "\n",
    "\n",
    "\n",
    "The best lack all conviction, while the worst   \n",
    "Are full of passionate \n",
    "intensity.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "539ae2df-560d-460e-888f-233072439fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(tokenize_sentwords_iter(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "debca818-1f3e-4419-a110-177f1f7816a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preload_vocab_syllabify(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ba4b266-0dd4-41f2-bda0-5dc07121322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_paras_df(txt_or_df,**kwargs):\n",
    "    if type(txt_or_df)==str:\n",
    "        \n",
    "        oiterr=tokenize_paras_ld(txt_or_df,**kwargs)\n",
    "        oiterr=tqdm(oiterr,desc='Iterating over paragraphs')\n",
    "        for para_i,para_d in enumerate(oiterr):\n",
    "            odf=tokenize_sentwords(para_d['para_str'],para_i=para_d['para_i'])\n",
    "            yield odf\n",
    "    elif type(txt_or_df)==pd.DataFrame:\n",
    "        for para_i,para_df in sorted(txt_or_df.groupby('para_i')):\n",
    "            yield para_df\n",
    "            \n",
    "def get_num_paras(txt_or_df,**kwargs):\n",
    "    if type(txt_or_df)==str:\n",
    "        paras=clean_text(txt_or_df).split(SEP_PARA)\n",
    "        return len(paras)\n",
    "    elif type(txt_or_df)==pd.DataFrame:\n",
    "        return len(set(getcol(txt_or_df,'para_i')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08b74895-8e0a-464d-a477-4aedb7179779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "268c1706-b475-401c-96da-f8559eefd07a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_token(toktxt):\n",
    "    return split_punct(toktxt.lower())[1]\n",
    "\n",
    "\n",
    "def get_word_from_lang(word_str,lang=DEFAULT_LANG,**kwargs):\n",
    "    func=CODE2LANG_SYLLABIFY[lang]\n",
    "    odf=func(word_str,**kwargs)\n",
    "    if not len(odf): return odf\n",
    "    assign_proms(odf)\n",
    "    return odf\n",
    "\n",
    "\n",
    "def get_word_tokdf(word_str,lang=DEFAULT_LANG,db=None,force=False,**kwargs):\n",
    "    # global WORD_TOKDF\n",
    "    \n",
    "    #if db is None: db=dc.Cache(PATH_DB_SYLLABIFY)\n",
    "    #with db as dbd:\n",
    "    word_tok=to_token(word_str)\n",
    "    key=(lang,word_tok)    \n",
    "    # with dc.Cache(PATH_DB_SYLLABIFY) as dbd:\n",
    "            \n",
    "    if not force and key in WORD_TOKDF: return WORD_TOKDF[key]\n",
    "    #with dc.Cache(PATH_DB_SYLLABIFY) as dbd:\n",
    "    #    if not force and key in dbd:\n",
    "    #        odf=from_blosc(dbd[key])\n",
    "    #        WORD_TOKDF[key]=odf\n",
    "    #        return odf\n",
    "    odf=get_word_from_lang(word_tok,lang=lang,**kwargs)\n",
    "    #dbd[key]=to_blosc(odf)\n",
    "    WORD_TOKDF[key]=odf\n",
    "    return odf\n",
    "    \n",
    "    \n",
    "    # if \n",
    "\n",
    "#     if not force and key in WORD_TOKDF:\n",
    "#         odf=WORD_TOKDF[key]\n",
    "#     # elif not force and key in dbd:\n",
    "        \n",
    "#     #if not force and key in dbd:\n",
    "#     #    odf=from_blosc(dbd[key])\n",
    "#         #WORD_TOKDF[key]=odf\n",
    "#     else:\n",
    "#         odf=get_word_from_lang(word_tok,lang=lang,**kwargs)\n",
    "#         WORD_TOKDF[key]=odf\n",
    "#         #dbd[key]=to_blosc(odf)\n",
    "    return odf\n",
    "\n",
    "def iter_words_tokdf(word_strs,lang=DEFAULT_LANG,force=False,progress=False,num_proc=1,**kwargs):\n",
    "    # global WORD_TOKDF\n",
    "    # with dc.Cache(PATH_DB_SYLLABIFY) dddas db:\n",
    "    #     oiterr=pmap_iter(\n",
    "    #         get_word_tokdf,\n",
    "    #         word_strs,\n",
    "    #         kwargs=dict(lang=lang,db=db),\n",
    "    #         num_proc=num_proc,\n",
    "    #         desc='Iterating over words'\n",
    "    #     )\n",
    "    #     yield from oiterr\n",
    "    oiterr=pmap_iter(\n",
    "        get_word_tokdf,\n",
    "        word_strs,\n",
    "        kwargs=dict(lang=lang),\n",
    "        num_proc=num_proc,\n",
    "        desc='Iterating over words',\n",
    "        progress=progress\n",
    "    )\n",
    "    yield from oiterr\n",
    "\n",
    "def get_uniqe_tokens(word_strs,**kwargs):\n",
    "    if type(word_strs)==str: word_strs=tokenize_fast(word_strs)\n",
    "    word_toks=list(set(list(map(to_token, word_strs))))\n",
    "    return word_toks\n",
    "        \n",
    "def get_words_tokdf(word_strs,**kwargs):\n",
    "    # word_toks=get_uniqe_tokens(word_strs,**kwargs)\n",
    "    o=list(iter_words_tokdf(word_strs,**kwargs))\n",
    "    return pd.concat(o) if len(o) else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982fa9d0-300f-40bf-a49c-5a49ba4d0ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# def tokenize_sents_txt(txt): return nltk.sent_tokenize(txt)\n",
    "# def tokenize_words_txt(txt): return tokenize_nice2(txt)\n",
    "\n",
    "# def tokenize_nlp_txt(\n",
    "#         txt,\n",
    "#         nlp=None,\n",
    "#         sentwords=None,\n",
    "#         pretokenized=True,\n",
    "#         lang=DEFAULT_LANG,\n",
    "#         processors=['tokenize'],\n",
    "#         **kwargs):\n",
    "#     #print(nlp,processors)\n",
    "#     if nlp is None:\n",
    "#         nlp=get_nlp(\n",
    "#             lang=lang,\n",
    "#             processors=processors,\n",
    "#             tokenize_pretokenized=pretokenized\n",
    "#         )\n",
    "    \n",
    "#     if pretokenized:\n",
    "#         return nlp(sentwords)\n",
    "#     else:\n",
    "#         return nlp(txt)\n",
    "\n",
    "# def clean_text(txt):\n",
    "#     return txt.replace('\\r\\n','\\n').replace('\\r','\\n')\n",
    "\n",
    "\n",
    "# def tokenize_paras_df(txt,**kwargs):\n",
    "#     return pd.DataFrame(tokenize_paras_ld(txt,**kwargs))\n",
    "\n",
    "\n",
    "# def do_tokenize_paras_nlp_iter(row,pretokenized=True,**kwargs):\n",
    "#     txt=row['para_str']\n",
    "#     tokdf=row['para_tokdf']=tokenize_sentwords(txt,**kwargs)\n",
    "#     sentwords=tokenize_sentwords_ll(tokdf)\n",
    "#     doc=tokenize_nlp_txt(\n",
    "#         txt,\n",
    "#         sentwords=sentwords,\n",
    "#         pretokenized=True,\n",
    "#         **kwargs\n",
    "#     )\n",
    "#     row['para_doc']=doc\n",
    "#     return row\n",
    "\n",
    "# def tokenize_paras_nlp_iter(\n",
    "#         txt,\n",
    "#         ld_paras=None,\n",
    "#         progress=True,\n",
    "#         num_proc=1,\n",
    "#         shuffle_paras=False,\n",
    "#         lim_paras=None,\n",
    "\n",
    "#         lang=DEFAULT_LANG,\n",
    "#         processors=DEFAULT_PROCESSORS,\n",
    "#         pretokenized=True,\n",
    "#         **kwargs):\n",
    "\n",
    "#     #from stanza_batch import batch    \n",
    "    \n",
    "#     if ld_paras is None: ld_paras=tokenize_paras_ld(txt,**kwargs)\n",
    "#     if shuffle_paras: random.shuffle(ld_paras)\n",
    "#     if lim_paras: ld_paras=ld_paras[:lim_paras]\n",
    "\n",
    "#     nlp=get_nlp(lang=lang,processors=processors,tokenize_pretokenized=pretokenized)\n",
    "#     kwargs['nlp']=nlp\n",
    "#     kwargs['lang']=lang\n",
    "#     kwargs['processors']=processors\n",
    "#     # kwargs['pretokenized']=pretokenized\n",
    "\n",
    "#     iterr=pmap_iter(\n",
    "#         do_tokenize_paras_nlp_iter,\n",
    "#         ld_paras,\n",
    "#         num_proc=num_proc,\n",
    "#         desc='Scanning paragraphs',\n",
    "#         kwargs=kwargs\n",
    "#     )\n",
    "#     yield from iterr\n",
    "\n",
    "\n",
    "# def tokenize_sentwords_iter(\n",
    "#         txt,\n",
    "#         doc=None,\n",
    "#         lang=DEFAULT_LANG,\n",
    "#         engine='',\n",
    "#         sep_line=SEP_LINE,\n",
    "#         sep_para=SEP_STANZA,\n",
    "#         seps_phrase=SEPS_PHRASE,\n",
    "#         processors=DEFAULT_PROCESSORS,\n",
    "#         progress=True,\n",
    "#         para_i=None,\n",
    "#         **kwargs):\n",
    "#     char_i=0\n",
    "#     line_i=1\n",
    "#     linepart_i=1\n",
    "#     linepart_ii=0\n",
    "\n",
    "#     #if not doc:\n",
    "#     #    doc=tokenize_nlp_txt(txt,processors=processors,lang=lang,**kwargs)\n",
    "    \n",
    "#     #sents=doc.sentences\n",
    "#     # start_offset=sents[0].tokens[0].start_char\n",
    "#     start_offset=0\n",
    "\n",
    "#     sents=tokenize_sents_txt(txt)\n",
    "\n",
    "#     for sent_i, sent in enumerate(sents):\n",
    "#         tokens=tokenize_words_txt(sent)\n",
    "#         for tok_i,tok in enumerate(tokens):\n",
    "#             #prefstr=txt[char_i-start_offset : tok.start_char - start_offset]\n",
    "#             # realtok=tok.text\n",
    "#             realtok=tok\n",
    "#             prefstr,wordstr,sufstr=split_punct(realtok)\n",
    "#             tokstr=wordstr+sufstr\n",
    "#             #tokstr=txt[tok.start_char - start_offset : tok.end_char - start_offset]\n",
    "#             #realtok=txt[char_i-start_offset:tok.end_char-start_offset]\n",
    "#             #print([prefstr,tokstr])\n",
    "#             if sep_line in prefstr and realtok.strip(): line_i+=1\n",
    "#             # char_i=tok.end_char\n",
    "#             #featd=tok.to_dict()[0]\n",
    "#             #print()\n",
    "#             odx_para=dict(para_i=para_i) if para_i is None else {}\n",
    "#             #odx_feat=dict(('word_'+k,v) for k,v in featd.items() if k not in badcols)\n",
    "#             odx_word=dict(\n",
    "#                 para_i=para_i,\n",
    "#                 sent_i=sent_i+1,\n",
    "#                 sentpart_i=linepart_i,\n",
    "#                 line_i=line_i,\n",
    "#                 word_i=tok_i+1,\n",
    "#                 word_pref=prefstr,\n",
    "#                 word_str=tokstr,\n",
    "#                 # word_str=wordstr,\n",
    "#                 # word_suf=sufstr,\n",
    "#                 # word_str=realtok,\n",
    "#                 word_tok=wordstr.lower(),\n",
    "#                 # word_str1=tok.text,\n",
    "#                 # word_str2=tokstr,\n",
    "#                 # word_tok3=realtok,\n",
    "#             )\n",
    "\n",
    "#             # assert tok.text==tokstr\n",
    "            \n",
    "#             # odx=dict(**odx_para, **odx_word, **odx_feat)\n",
    "#             #print(odx)\n",
    "#             yield odx_word\n",
    "            \n",
    "#             # if change_linepart:\n",
    "#                 # linepart_i+=1\n",
    "#                 # change_linepart=False\n",
    "#             if set(realtok)&set(seps_phrase):\n",
    "#                 linepart_i+=1\n",
    "#                 # change_linepart=True\n",
    "\n",
    "# def tokenize_sentwords(txt,doc=None,**kwargs):\n",
    "#     iterr=tokenize_sentwords_iter(txt,doc=doc,**kwargs)\n",
    "#     tokdf=pd.DataFrame(iterr)\n",
    "#     return tokdf\n",
    "\n",
    "# def tokenize_sentwords_ll(tokdf,**kwargs):\n",
    "#     if not len(tokdf): return []\n",
    "#     return [list(sdf.word_str) for si,sdf in sorted(tokdf.groupby('sent_i'))]\n",
    "\n",
    "# def tokenize_nlp_iter(\n",
    "#         txt,\n",
    "#         constituency=True,\n",
    "#         depparse=True,\n",
    "#         syllabify=True,\n",
    "#         num_proc=1,\n",
    "#         **kwargs):\n",
    "#     # make proc list\n",
    "    \n",
    "#     processors=get_processors(constituency=constituency,depparse=depparse,**kwargs)\n",
    "\n",
    "#     iterr=tokenize_paras_nlp_iter(txt,processors=processors,num_proc=num_proc,**kwargs)\n",
    "    \n",
    "#     for para_row in iterr:\n",
    "#         para_str,para_doc=para_row['para_str'], para_row['para_doc']\n",
    "        \n",
    "#         # tokenize\n",
    "#         tokdf=tokenize_sentwords(para_str,doc=para_doc,**kwargs)\n",
    "        \n",
    "#         # add anno?\n",
    "#         if constituency: tokdf=tokenize_constituency(tokdf,para_doc,**kwargs)\n",
    "#         if depparse: tokdf=tokenize_deps(tokdf,para_doc,**kwargs)\n",
    "#         if syllabify: tokdf=syllabify_df(tokdf,**kwargs)\n",
    "\n",
    "#         for k in ['para_i']: tokdf[k]=para_row[k]\n",
    "        \n",
    "#         # done\n",
    "#         odf=setindex(tokdf)\n",
    "#         odf.attrs=dict(para_row)\n",
    "#         yield odf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def tokenize_constituency(tokdf,doc,**kwargs):\n",
    "#     sents=doc.sentences\n",
    "#     ld=[]\n",
    "#     sentd_orig=dict((get_sent_id_tokens(sent), sent) for sent in sents)\n",
    "#     for sent_i, sent in enumerate(sents):\n",
    "#         # print('????',sent_i)\n",
    "#         # print(sent.constituency)\n",
    "#         # sent_id_tokens=get_sent_id_tokens(sent)\n",
    "#         try:\n",
    "#             sent_id_constituency=get_sent_id_constituency(sent)\n",
    "#             sent_orig=sentd_orig[sent_id_constituency]        \n",
    "#         except (KeyError,AttributeError) as e:\n",
    "#             # print('!! Cannot find sentence id',e)\n",
    "#             continue\n",
    "\n",
    "#         senttree=recurse_tree(sent.constituency, node_i=0, path=[])\n",
    "#         for word_i,word_constituency_path in enumerate(senttree):\n",
    "#             word_constituency_path_str='('.join(word_constituency_path)\n",
    "#             constituency_depth=len(word_constituency_path)\n",
    "#             dx={\n",
    "#                 'sent_i': sent_orig.id+1,\n",
    "#                 'word_i': word_i+1,\n",
    "#                 'word_depth':constituency_depth,\n",
    "#                 'word_constituency':word_constituency_path_str,\n",
    "#             }\n",
    "#             ld.append(dx)\n",
    "#         # stop\n",
    "#     df=pd.DataFrame(ld)\n",
    "#     if not len(df): return tokdf\n",
    "#     return tokdf.merge(df,on=['sent_i','word_i'],how='left')\n",
    "\n",
    "# def tokenize_deps(tokdf,doc,cols_done=set(),lang=DEFAULT_LANG,**kwargs):\n",
    "#     sents=doc.sentences\n",
    "#     ld=[]\n",
    "#     cols_done=set(tokdf.columns)\n",
    "#     for sent_i, sent in enumerate(sents):\n",
    "#         for word_i,word in enumerate(sent.tokens):\n",
    "#             feats=word.to_dict()[0]\n",
    "#             statd=dict((f'word_{k}',v) for k,v in feats.items() if k not in badcols)\n",
    "#             for feat in feats.get('feats','').split('|'):\n",
    "#                 if not feat: continue\n",
    "#                 fk,fv=feat.split('=',1)\n",
    "#                 statd[fk]=fv\n",
    "            \n",
    "#             dx={\n",
    "#                 'sent_i': sent.id+1,\n",
    "#                 'word_i': word_i+1,\n",
    "#                 **statd\n",
    "#             }\n",
    "#             ld.append(dx)\n",
    "#     df=pd.DataFrame(ld).fillna('')\n",
    "#     joiner=['sent_i','word_i']\n",
    "#     ocols=(set(df.columns)-set(tokdf.columns))|set(joiner)\n",
    "#     return tokdf.merge(df[ocols],on=joiner,how='left')\n",
    "\n",
    "\n",
    "\n",
    "# ########\n",
    "# # Scanning\n",
    "# ########\n",
    "\n",
    "# def scan_iter_stanzanlp(txt,**kwargs):\n",
    "#     iterr=tokenize_nlp_iter(txt,**kwargs)\n",
    "#     yield from iterr\n",
    "\n",
    "\n",
    "# def scan_iter(txt,**kwargs): return scan_iter_stanzanlp(txt,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34c0a5b4-4b54-4339-8d73-a11b6c32b25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_words_tokdf(novel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73179b0f-01dc-4982-8e49-82e48725f4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_words_tokdf(txt.split() + txt.split() + txt.split(), force=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd8bf6fc-5984-4fcd-ab83-cce24ddb0ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllabify_df(idf,num_proc=1,lang=DEFAULT_LANG,**kwargs):\n",
    "    odf = get_words_tokdf(\n",
    "        get_uniqe_tokens(\n",
    "            idf.word_tok,\n",
    "            progress=True,\n",
    "            **kwargs\n",
    "        )\n",
    "    )\n",
    "    if not len(odf): return idf\n",
    "    return idf.merge(odf,on='word_tok',how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b98a5536-1a2e-4208-a2e5-84319d69af64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_cmu=list(english.get_cache([english.CMU_DICT_FN]).keys())\n",
    "# len(words_cmu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a40b0326-04f8-4ef7-a5b1-1ce43239f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preload_vocab(words,force=False,**kwargs):\n",
    "    tokens=get_uniqe_tokens(words)\n",
    "    oiterr=iter_words_tokdf(tokens,force=force,**kwargs)\n",
    "    # oiterr=tqdm(oiterr,total=len(tokens),desc='Initializing vocabulary')\n",
    "    list(oiterr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "336e7752-c241-47d6-8a5a-041734f5a6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scan_iter_txt(txt_or_df,syllabify=True,progress=True,**kwargs):\n",
    "    # if syllabify:\n",
    "        # preload_vocab(txt_or_df,progress=True)\n",
    "    oiterr=iter_paras_df(txt_or_df)\n",
    "    # if progress: oiterr=tqdm(oiterr,total=get_num_paras(txt_or_df))\n",
    "    for df_para in oiterr:\n",
    "        if syllabify: df_para=syllabify_df(df_para)\n",
    "        # yield setindex(df_para)\n",
    "        yield df_para\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf4f2733-d8cd-4af4-a252-ae484b8f52e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df in scan_iter_txt(novel,syllabify=False): break\n",
    "# # syllabify_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcc57d09-0e61-41ea-8865-c2345c1abafa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scan_iter_txt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/py/jyzw5nyj1fnf0c_1czgsg2fc0000gn/T/ipykernel_23811/3417647902.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# %%timeit -n1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscan_iter_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnovel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'scan_iter_txt' is not defined"
     ]
    }
   ],
   "source": [
    "# %%timeit -n1\n",
    "for x in scan_iter_txt(novel): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70858e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = MetricalTree.convert(t)\n",
    "                t.set_lstress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0fb01c2a-9188-4350-9651-cfc5ca381e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setindex(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ae3c7fb0-1687-4596-8a6c-3e903807b5b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/py/jyzw5nyj1fnf0c_1czgsg2fc0000gn/T/ipykernel_19555/3957423419.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759e26f8-45e8-42ed-a3fe-915906029e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d15e75-6ba6-40c5-ab20-da4b5f58e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preload_vocab(novel,force=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231cf7da-5dab-4673-99c2-7c21249a6b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2a79b4-7258-46ef-bd19-fda80593f594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c963ea2-feb1-4076-aa89-788a18cf7c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preload_vocab(novel,force=False,num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876377de-67f9-42b9-a5ae-b1404746bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preload_vocab(words_cmu,force=False,num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eafbfa1-c87c-4426-b48f-c5b001680fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in iter_words_tokdf(novel): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c1f2b7-4973-4af8-b255-7582414ec14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ab8bd6-f88a-4742-908b-22a8ed282538",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dfpara in scan_iter_txt(novel,syllabify=False): break\n",
    "dfpara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cbe59c-0cba-45d5-80da-eb6daeb47968",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dfpara in scan_iter_txt(novel,syllabify=True): pass\n",
    "dfpara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade77e3c-78f4-4c02-91f6-6bf4d014e167",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dfpara in scan_iter_txt(txt,syllabify=True): pass\n",
    "dfpara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1007304-4164-4d0f-bb7e-ffcbe2f4bc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# syllabify_df(dfpara)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28629d8f-33f9-4797-81ac-3ce1add8cf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in scan_iter_txt(novel,syllabify=False): pass\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db51672e-3d9a-4d2a-9b63-899afbd53cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=list(scan_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e967511-92ee-4cd7-8682-dd627dc9dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_parse_nlp_docs(\n",
    "        docs,\n",
    "        nlp=None,\n",
    "        num_proc=1,\n",
    "        **kwargs):\n",
    "    ## nlp\n",
    "    # if not len(docs): yield docs\n",
    "    if num_proc>1:\n",
    "        with mp.Pool(num_proc) as pool: # This is the fastest. joblib(thread, mp) experimented.\n",
    "            yield from pool.imap(nlp, docs)\n",
    "    else:\n",
    "        for doc in docs:\n",
    "            yield nlp(doc) if nlp is not None else doc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc65156-db7e-47aa-977b-54c573149308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_nlp_doc(tokdf,doc,**kwargs):\n",
    "    sents=doc.sentences\n",
    "    ld=[]\n",
    "    cols_done=set(tokdf.columns)\n",
    "    for sent_i, sent in enumerate(sents):\n",
    "        for word_i,word in enumerate(sent.tokens):\n",
    "            feats=word.to_dict()[0]\n",
    "            statd=dict((f'word_{k}',v) for k,v in feats.items() if k not in badcols)\n",
    "            for feat in feats.get('feats','').split('|'):\n",
    "                if not feat: continue\n",
    "                fk,fv=feat.split('=',1)\n",
    "                statd[fk]=fv\n",
    "            \n",
    "            dx={\n",
    "                'sent_i': sent.id+1,\n",
    "                'word_i': word_i+1,\n",
    "                **statd\n",
    "            }\n",
    "            ld.append(dx)\n",
    "    df=pd.DataFrame(ld).fillna('')\n",
    "    joiner=['sent_i','word_i']\n",
    "    ocols=(set(df.columns)-set(tokdf.columns))|set(joiner)\n",
    "    return tokdf.merge(df[ocols],on=joiner,how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcae624-2f30-4c87-bfd3-2e34fa577d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddff835-4ff2-4dfb-a53f-a304a14f513b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc95bbc-a582-4197-b7bf-93c977ea30da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e658b9-2415-46d0-9332-43b9d8a5b59e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619aa850-c3f8-458d-83e4-cd05275f0603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_iter_nlp(\n",
    "        txt,\n",
    "        nlp=None,\n",
    "        paras_lim=None,\n",
    "        progress=True,\n",
    "        num_proc=1,\n",
    "    \n",
    "        lang=DEFAULT_LANG,\n",
    "        paras_shuffle=False,\n",
    "        \n",
    "        postag=False,\n",
    "        constituency=False,\n",
    "        depparse=False,\n",
    "        syllabify=True,\n",
    "    \n",
    "        **kwargs):\n",
    "\n",
    "    ## prep documents\n",
    "    paras_ld=tokenize_paras_ld(txt)\n",
    "    if paras_shuffle: random.shuffle(paras_ld)\n",
    "    if paras_lim: paras_ld=paras_ld[:paras_lim]\n",
    "    \n",
    "    para_dfs=[\n",
    "        tokenize_sentwords(para_d['para_str'],para_i=para_d['para_i'])\n",
    "        for para_d in tqdm(paras_ld,desc='Tokenizing paragraphs')\n",
    "    ]\n",
    "    \n",
    "    para_doclls=[\n",
    "        tokenize_sentwords_ll(para_df)\n",
    "        for para_df in tqdm(para_dfs,desc='Tokenizing sentences and words')\n",
    "    ]\n",
    "    \n",
    "    processors=get_processors(\n",
    "        postag=postag,\n",
    "        constituency=constituency,\n",
    "        depparse=depparse,\n",
    "    )\n",
    "    \n",
    "    if processors and nlp is None:\n",
    "        nlp = get_nlp(\n",
    "            lang=lang,\n",
    "            pretokenized=True,\n",
    "            processors=processors\n",
    "        )\n",
    "        \n",
    "    # iter docs\n",
    "    doc_iter = iter_parse_nlp_docs(\n",
    "        para_doclls,\n",
    "        nlp=nlp,\n",
    "        lang=lang,\n",
    "        constituency=constituency,\n",
    "        depparse=depparse,\n",
    "        num_proc=num_proc,\n",
    "        progress=False\n",
    "    )\n",
    "    \n",
    "    oiterr=zip(paras_ld,para_dfs,doc_iter)\n",
    "    \n",
    "    if progress:\n",
    "        oiterr=tqdm(\n",
    "            oiterr,\n",
    "            total=len(para_doclls),\n",
    "            desc=f'Tokenizing NLP [x{num_proc}]'\n",
    "        )\n",
    "    \n",
    "    # yield from oiterr\n",
    "    for para_d,para_tokdf,para_doc in oiterr:\n",
    "        #if postag or constituency or depparse:\n",
    "        if processors:\n",
    "            para_tokdf = tokenize_nlp_doc(para_tokdf, para_doc, **kwargs)\n",
    "        \n",
    "        if constituency:\n",
    "            para_tokdf = tokenize_constituency(para_tokdf,para_doc,**kwargs)\n",
    "        \n",
    "        if syllabify:\n",
    "            para_tokdf=syllabify_df(para_tokdf,**kwargs)\n",
    "        \n",
    "        yield setindex(para_tokdf.assign(para_i=para_d['para_i']))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510525cc-b641-4d96-8f0e-c73376e06cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oiter=scan_iter_nlp(\n",
    "#     txt,\n",
    "#     paras_lim=100,\n",
    "#     # syllabify=True,\n",
    "#     postag=True,\n",
    "#     depparse=True,\n",
    "#     constituency=True,\n",
    "#     syllabify=True,\n",
    "#     num_proc=1\n",
    "# )\n",
    "# for scanned_para_df in oiter: pass\n",
    "# scanned_para_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17127420-1221-4cb7-a16e-57ac89279e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_iter(txt,groupby='para',**kwargs):\n",
    "    if 'syllabify' not in kwargs: kwargs['syllabify']=True\n",
    "    for df_para in scan_iter_nlp(txt,**kwargs):\n",
    "        \n",
    "        grpr=None\n",
    "        if groupby=='sent':\n",
    "            grpr='sent_i'\n",
    "        elif groupby=='sentpart':\n",
    "            grpr=['sent_i','sentpart_i']\n",
    "        elif groupby=='word':\n",
    "            grpr=['sent_i','word_i']\n",
    "        elif groupby=='syll':\n",
    "            grpr=['sent_i','word_i','word_ipa_i','syll_i']\n",
    "            \n",
    "        if grpr is None:\n",
    "            yield df_para\n",
    "        else:\n",
    "            for gi,dfg in sorted(df_para.groupby(grpr)):\n",
    "                yield dfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5cbadc-b62c-4446-9446-83e83a5b7a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(scan_iter(txt,groupby='sentpart'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35e6972-6e6e-4311-9020-fbc2a21a64ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57b725e-c1e0-4cbc-8011-07dd9ac32d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cba2cc1-30a2-421a-b364-d10adbc4ae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "    \n",
    "ld=tokenize_paras_ld(txt)\n",
    "ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d144de47-f2d7-4717-a25c-faac9b18f810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce557a6-9517-42d6-88d8-3fc5475469e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df1340-6bc4-48eb-83a1-b4a099b8166f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94da742c-d194-415d-a70f-0e3dfeceda2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "list(iter_paras_df(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e4774e-2c0b-4b1c-9f26-5915f996e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "txtdf=pd.concat(iter_paras_df(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a0f7dd-8c6d-4b1a-a309-4acaa3924611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# txtdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecf494b-7566-4145-a2b4-ec8e2f940b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "list(iter_paras_df(txtdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a77a9b7-a4e7-4233-b4b9-035bc86bc8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab607e-f05a-4c6e-8bfb-ec5e7a9440af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentwords_ll(tokdf,**kwargs):\n",
    "    if not len(tokdf): return []\n",
    "    return [list(getcol(sdf,'word_str')) for si,sdf in sorted(tokdf.groupby('sent_i'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b36808-dfda-4665-8456-a809743fee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_paras_nlp(\n",
    "        txt_or_tokdf,\n",
    "        nlp=None,\n",
    "        constituency=False,\n",
    "        depparse=False,\n",
    "        progress=True,\n",
    "        syllabify=False,\n",
    "        num_proc=1,\n",
    "        lang=DEFAULT_LANG,\n",
    "        **kwargs):\n",
    "    \n",
    "    # get nlp\n",
    "    if nlp is None:\n",
    "        processors=get_processors(constituency=constituency,depparse=depparse)\n",
    "        print(processors)\n",
    "        nlp=get_nlp(lang=lang, procesors=processors)\n",
    "    \n",
    "    oiterr=iter_paras_df(txt_or_tokdf,**kwargs)\n",
    "    if progress: oiterr=tqdm(oiterr,total=get_num_paras(txt_or_tokdf))\n",
    "    for para_df in oiterr:\n",
    "        sentwords_ll = tokenize_sentwords_ll(para_df)\n",
    "        doc = nlp(sentwords_ll)\n",
    "        para_tokdf = tokenize_nlp_doc(para_df, doc, lang=lang, **kwargs)\n",
    "        if constituency: para_tokdf = tokenize_constituency(para_tokdf,doc,**lang)\n",
    "        if syllabify: para_tokdf=syllabify_df(para_tokdf,**kwargs)\n",
    "        yield setindex(para_tokdf)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db58df70-0dca-44c7-ae61-ef7c3d72b30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pdf in iter_paras_nlp(txt): pass\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad312f-22d6-464e-bdbe-c2a5fcdf6dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def scan_iter_nlp(\n",
    "#         txt_or_tokdf,\n",
    "#         nlp=None,\n",
    "#         constituency=False,\n",
    "#         depparse=False,\n",
    "#         syllabify=False,\n",
    "#         num_proc=1,\n",
    "#         custom_tokenize=True,\n",
    "#         lang=DEFAULT_LANG,\n",
    "#         **kwargs):\n",
    "    \n",
    "#     # make orig tokdf\n",
    "#     tokdf=tokenize_parasentword(txt_or_tokdf,**kwargs) if type(txt_or_tokdf)==str else txt_or_tokdf    \n",
    "    \n",
    "#     # get nlp\n",
    "#     if nlp is None:\n",
    "#         processors=get_processors(constituency=constituency,depparse=depparse,**kwargs)\n",
    "#         nlp=get_nlp(lang=lang, procesors=processors,custom_tokenize=custom_tokenize)\n",
    "    \n",
    "#     objs=[(paradf,nlp) for para_i,paradf in sorted(tokdf.groupby('para_i'))]\n",
    "#     kwargs=dict(constituency=constituency,depparse=depparse,syllabify=syllabify,**kwargs)\n",
    "#     oiter=pmap_iter(do_scan_iter_nlp, objs, num_proc=num_proc, kwargs=kwargs)\n",
    "    \n",
    "#     yield from oiter\n",
    "    \n",
    "    \n",
    "    \n",
    "# def do_scan_iter_nlp(\n",
    "#         obj,\n",
    "#         constituency=False,\n",
    "#         depparse=False,\n",
    "#         syllabify=False,                 \n",
    "#         **kwargs):\n",
    "    \n",
    "#     tokdf,nlp=obj\n",
    "#     sentwords=tokenize_sentwords_ll(tokdf)\n",
    "#     doc=nlp(sentwords)\n",
    "#     para_str,para_doc=para_row['para_str'], para_row['para_doc']\n",
    "\n",
    "#     # add anno?\n",
    "#     if constituency: tokdf=tokenize_constituency(tokdf,para_doc,**kwargs)\n",
    "#     if depparse: tokdf=tokenize_deps(tokdf,para_doc,**kwargs)\n",
    "#     if syllabify: tokdf=syllabify_df(tokdf,**kwargs)\n",
    "\n",
    "#     for k in ['para_i']: tokdf[k]=para_row[k]\n",
    "\n",
    "#     # done\n",
    "#     odf=setindex(tokdf)\n",
    "#     odf.attrs=dict(para_row)\n",
    "#     return tokdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf22dc3-2c57-42f5-942d-9f3359807cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(scan_iter_nlp(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26ca80e-2226-4886-8095-7a0fe698a7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(scan_iter(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c2ef65-35ff-4db7-93b4-7c5252084ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ac0847-2bce-4109-af74-0f91d9bef6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=list(scan_iter(txt,num_proc=1,lim_paras=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e310ac22-f4f8-493b-a1c3-0575c664e233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0ef6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for para in scan_iter(txt,\n",
    "                      num_proc=1,\n",
    "                      lim_paras=None,\n",
    "                      shuffle=True,\n",
    "                      depparse=False,\n",
    "                      constituency=False): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3737adb2-5679-4ba0-88cf-cf95c0e3a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# para.tail(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e7ae8d-690b-4530-977a-d45b88c38e20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d9a62951c4de3cec93df06e5a8769682e2513316501195b5ad08e283a24e7b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "language": "python",
   "name": "python3811jvsc74a57bd08d9a62951c4de3cec93df06e5a8769682e2513316501195b5ad08e283a24e7b2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
